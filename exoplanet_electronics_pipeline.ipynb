{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8198db62",
   "metadata": {},
   "source": [
    "# KOI Ensemble Pipeline (Electronics paper reproduction)\n",
    "\n",
    "This notebook implements a tabular KOI pipeline inspired by the Electronics (2024) ensemble-based study. It:\n",
    "\n",
    "- Loads a local KOI CSV (you set the path in the first code cell).\n",
    "- Preprocesses the table, selects numeric features, and encodes a binary label (candidate/confirmed per your choice).\n",
    "- Trains ensemble classifiers: RandomForest, ExtraTrees, AdaBoost (with decision trees), Random Subspace (Bagging with limited features), and Stacking (logistic meta).\n",
    "- Evaluates models and records metrics: **accuracy, train time, test time, inference time (per sample), estimated FLOPs during training (approx), precision, recall, F1, ROC AUC**.\n",
    "- Saves model artifacts to `models/` and plots to `plots/<model_name>/`.\n",
    "\n",
    "**Caveat about FLOPS**: Exact FLOPS is hardware-dependent and requires low-level profiling. This notebook provides a **practical, explainable estimate**: for tree-based models we approximate total work as the sum over all internal nodes of (node_sample_count * n_features) â€” i.e., the number of feature comparisons evaluated while searching splits. For XGBoost trees we use per-node `cover` values from the booster as a proxy for sample counts. This is an approximation and should be treated as an index of computational work rather than an exact measured FLOPS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51db62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install dependencies if needed (comment out if already installed)\n",
    "import sys\n",
    "!{sys.executable} -m pip install --quiet pandas scikit-learn xgboost matplotlib seaborn imbalanced-learn joblib openpyxl\n",
    "print('Dependencies ready.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6f73d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOI_CSV = data/kepler_koi.csv\n",
      "Output folders: models/, plots/, results/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==== EDIT THESE PATHS ====\n",
    "# Set KOI CSV path (local file). If you have TOI/K2 also, you may add them, but Electronics paper used KOI.\n",
    "KOI_CSV = 'data/kepler_koi.csv'  # <<-- edit this path to your local KOI csv\n",
    "\n",
    "# Output directories\n",
    "import os\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('plots', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "print('KOI_CSV =', KOI_CSV)\n",
    "print('Output folders: models/, plots/, results/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "03c75d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded KOI: (9564, 8)\n",
      "Columns (first 50): ['kepoi_name', 'koi_disposition', 'koi_pdisposition', 'koi_period', 'koi_duration', 'koi_prad', 'koi_depth', 'koi_model_snr']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kepoi_name</th>\n",
       "      <th>koi_disposition</th>\n",
       "      <th>koi_pdisposition</th>\n",
       "      <th>koi_period</th>\n",
       "      <th>koi_duration</th>\n",
       "      <th>koi_prad</th>\n",
       "      <th>koi_depth</th>\n",
       "      <th>koi_model_snr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K00752.01</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>9.488036</td>\n",
       "      <td>2.9575</td>\n",
       "      <td>2.26</td>\n",
       "      <td>615.8</td>\n",
       "      <td>35.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K00752.02</td>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>54.418383</td>\n",
       "      <td>4.5070</td>\n",
       "      <td>2.83</td>\n",
       "      <td>874.8</td>\n",
       "      <td>25.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K00753.01</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>CANDIDATE</td>\n",
       "      <td>19.899140</td>\n",
       "      <td>1.7822</td>\n",
       "      <td>14.60</td>\n",
       "      <td>10829.0</td>\n",
       "      <td>76.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  kepoi_name koi_disposition koi_pdisposition  koi_period  koi_duration  \\\n",
       "0  K00752.01       CONFIRMED        CANDIDATE    9.488036        2.9575   \n",
       "1  K00752.02       CONFIRMED        CANDIDATE   54.418383        4.5070   \n",
       "2  K00753.01       CANDIDATE        CANDIDATE   19.899140        1.7822   \n",
       "\n",
       "   koi_prad  koi_depth  koi_model_snr  \n",
       "0      2.26      615.8           35.8  \n",
       "1      2.83      874.8           25.8  \n",
       "2     14.60    10829.0           76.3  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load KOI CSV and minimal preprocessing\n",
    "import pandas as pd, numpy as np, os\n",
    "if not os.path.exists(KOI_CSV):\n",
    "    raise FileNotFoundError(f'KOI CSV not found at {KOI_CSV}. Edit the path in the notebook and re-run.')\n",
    "\n",
    "df = pd.read_csv(KOI_CSV)\n",
    "print('Loaded KOI:', df.shape)\n",
    "print('Columns (first 50):', df.columns.tolist()[:50])\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "586be850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded KOI: (9564, 8)\n",
      "Columns (first 50): ['kepoi_name', 'koi_disposition', 'koi_pdisposition', 'koi_period', 'koi_duration', 'koi_prad', 'koi_depth', 'koi_model_snr']\n",
      "Using label column: koi_disposition\n",
      "Task: Planet (CANDIDATE/CONFIRMED) vs FALSE POSITIVE\n",
      "After label mapping, shape = (9564, 9)\n",
      "label\n",
      "0    4839\n",
      "1    4725\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Selected 5 features (filtered to avoid leakage)\n",
      "Features: ['koi_period', 'koi_duration', 'koi_prad', 'koi_depth', 'koi_model_snr']\n",
      "\n",
      "Final dataset shape: (9564, 6)\n",
      "\n",
      "y shape: (9564,) (should be 1D)\n",
      "y unique values: [1 0]\n",
      "y value counts:\n",
      "label\n",
      "0    4839\n",
      "1    4725\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train: (6694, 5), Test: (2870, 5)\n",
      "Train class balance: [3387 3307]\n",
      "Test class balance: [1452 1418]\n"
     ]
    }
   ],
   "source": [
    "# Load KOI CSV and minimal preprocessing\n",
    "import pandas as pd, numpy as np, os\n",
    "\n",
    "df = pd.read_csv(KOI_CSV)\n",
    "print('Loaded KOI:', df.shape)\n",
    "print('Columns (first 50):', df.columns.tolist()[:50])\n",
    "\n",
    "# LABEL MAPPING - Use planet vs non-planet (more meaningful task)\n",
    "LABEL_STRATEGY = 'planet_vs_not'  # More realistic: planet vs false positive\n",
    "\n",
    "label_col = None\n",
    "for c in ['koi_disposition', 'koi_pdisposition']:\n",
    "    if c in df.columns:\n",
    "        label_col = c\n",
    "        break\n",
    "if label_col is None:\n",
    "    raise ValueError('Could not find a KOI label column.')\n",
    "print('Using label column:', label_col)\n",
    "\n",
    "working = df.copy()\n",
    "working[label_col] = working[label_col].astype(str).str.upper()\n",
    "\n",
    "if LABEL_STRATEGY == 'planet_vs_not':\n",
    "    # Planet (1) vs False Positive (0)\n",
    "    mapping = {'CANDIDATE':1, 'CONFIRMED':1, 'FALSE POSITIVE':0}\n",
    "    working['label'] = working[label_col].map(mapping)\n",
    "    working = working[working['label'].notna()].copy()\n",
    "    print('Task: Planet (CANDIDATE/CONFIRMED) vs FALSE POSITIVE')\n",
    "else:\n",
    "    raise ValueError('Use planet_vs_not strategy')\n",
    "\n",
    "print('After label mapping, shape =', working.shape)\n",
    "print(working['label'].value_counts())\n",
    "\n",
    "# CRITICAL: Feature selection - ONLY use observable/measurable features\n",
    "# Exclude ANY features that are derived from or hint at the disposition\n",
    "\n",
    "# Features that are SAFE (directly observable):\n",
    "safe_prefixes = [\n",
    "    'koi_period',      # Orbital period\n",
    "    'koi_time0bk',     # Transit epoch\n",
    "    'koi_impact',      # Impact parameter\n",
    "    'koi_duration',    # Transit duration\n",
    "    'koi_depth',       # Transit depth\n",
    "    'koi_prad',        # Planet radius\n",
    "    'koi_teq',         # Equilibrium temperature\n",
    "    'koi_insol',       # Insolation flux\n",
    "    'koi_model_snr',   # Signal to noise\n",
    "    'koi_steff',       # Stellar effective temperature\n",
    "    'koi_slogg',       # Stellar surface gravity\n",
    "    'koi_srad',        # Stellar radius\n",
    "    'ra', 'dec',       # Coordinates\n",
    "    'koi_kepmag'       # Kepler magnitude\n",
    "]\n",
    "\n",
    "# Features to EXCLUDE (these leak information about disposition):\n",
    "exclude_patterns = [\n",
    "    'kepoi', 'kepid', 'name', 'id', 'rowid',  # Identifiers\n",
    "    'score',           # Disposition scores\n",
    "    'fpflag',          # False positive flags\n",
    "    'disposition',     # Any disposition column\n",
    "    'comment',         # Comments\n",
    "    'datalink',        # Links\n",
    "    'pdisposition',    # Pipeline disposition\n",
    "]\n",
    "\n",
    "# Select features\n",
    "numeric = working.select_dtypes(include=[np.number]).copy()\n",
    "FEATURE_COLS = []\n",
    "\n",
    "for col in numeric.columns:\n",
    "    col_lower = col.lower()\n",
    "    \n",
    "    # Skip if matches exclude pattern\n",
    "    if any(pattern in col_lower for pattern in exclude_patterns):\n",
    "        continue\n",
    "    \n",
    "    # Keep if matches safe prefix OR has low missingness\n",
    "    if any(col_lower.startswith(prefix) for prefix in safe_prefixes):\n",
    "        if numeric[col].notna().mean() >= 0.5:  # At least 50% non-missing\n",
    "            FEATURE_COLS.append(col)\n",
    "\n",
    "print(f'\\nSelected {len(FEATURE_COLS)} features (filtered to avoid leakage)')\n",
    "print('Features:', FEATURE_COLS[:30])\n",
    "\n",
    "if len(FEATURE_COLS) < 5:\n",
    "    print('\\nWARNING: Very few features selected. Check your column names.')\n",
    "    print('Available numeric columns:', numeric.columns.tolist()[:50])\n",
    "\n",
    "# Prepare model dataframe\n",
    "model_df = working[['label'] + FEATURE_COLS].copy()\n",
    "print(f'\\nFinal dataset shape: {model_df.shape}')\n",
    "\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = model_df[FEATURE_COLS].copy()\n",
    "y = model_df['label'].astype(int).copy()  # This is already 1D\n",
    "\n",
    "# Verify y is 1D\n",
    "print(f'\\ny shape: {y.shape} (should be 1D)')\n",
    "print(f'y unique values: {y.unique()}')\n",
    "print(f'y value counts:\\n{y.value_counts()}')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f'\\nTrain: {X_train.shape}, Test: {X_test.shape}')\n",
    "print(f'Train class balance: {np.bincount(y_train)}')\n",
    "print(f'Test class balance: {np.bincount(y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80029e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Feature selection: auto-select numeric, drop identifier-like names, drop >50% missing\n",
    "# ignore_tokens = ['kepoi','kepid','name','id','rowid']\n",
    "# numeric = working.select_dtypes(include=[np.number]).copy()\n",
    "# drop_cols = [c for c in numeric.columns if any(tok in c.lower() for tok in ignore_tokens)]\n",
    "# numeric = numeric.drop(columns=[c for c in drop_cols if c in numeric.columns])\n",
    "# miss_frac = numeric.isna().mean()\n",
    "# keep_cols = miss_frac[miss_frac <= 0.5].index.tolist()\n",
    "# FEATURE_COLS = keep_cols\n",
    "# print('Selected', len(FEATURE_COLS), 'numeric feature columns')\n",
    "# print(FEATURE_COLS[:50])\n",
    "\n",
    "# model_df = working[['label'] + FEATURE_COLS].copy()\n",
    "# model_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "052187af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results/label_counts.csv and results/feature_descriptions.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Quick EDA: save stats and correlation plots\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "model_df['label'].value_counts().to_csv('results/label_counts.csv')\n",
    "model_df.describe().transpose().to_csv('results/feature_descriptions.csv')\n",
    "print('Saved results/label_counts.csv and results/feature_descriptions.csv')\n",
    "\n",
    "# Correlation heatmap for up to 30 features\n",
    "top_feats = model_df[FEATURE_COLS].var().sort_values(ascending=False).index[:30].tolist()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(model_df[top_feats].corr(), cmap='coolwarm')\n",
    "plt.title('Top features correlation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/correlation_top30.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "052fa19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (6694, 5) Test: (2870, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = model_df[FEATURE_COLS].copy()\n",
    "y = model_df['label'].astype(int).copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n",
    "print('Train:', X_train.shape, 'Test:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c6bb007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helpers ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Helpers: timing, inference time measurement, FLOPS estimates\n",
    "import time, numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# measure inference time\n",
    "def measure_inference_time(model, X, repeats=3):\n",
    "    func = model.predict_proba if hasattr(model, 'predict_proba') else model.predict\n",
    "    # warmup\n",
    "    try:\n",
    "        _ = func(X.iloc[:min(10,len(X))])\n",
    "    except Exception:\n",
    "        _ = model.predict(X.iloc[:min(10,len(X))])\n",
    "    times = []\n",
    "    for _ in range(repeats):\n",
    "        t0 = time.perf_counter()\n",
    "        _ = func(X)\n",
    "        t1 = time.perf_counter()\n",
    "        times.append(t1-t0)\n",
    "    median = np.median(times)\n",
    "    return median, median / len(X)\n",
    "\n",
    "# Estimate FLOPS for sklearn tree ensembles using node sample counts\n",
    "import numpy as np\n",
    "\n",
    "def estimate_flops_sklearn_ensemble(ensemble, n_features):\n",
    "    total = 0.0\n",
    "    try:\n",
    "        if hasattr(ensemble, 'estimators_'):\n",
    "            for est in ensemble.estimators_:\n",
    "                tree = getattr(est, 'tree_', None)\n",
    "                if tree is None:\n",
    "                    continue\n",
    "                node_samples = getattr(tree, 'n_node_samples', None)\n",
    "                if node_samples is None:\n",
    "                    node_count = getattr(tree, 'node_count', 0)\n",
    "                    node_samples = np.full(node_count, X_train.shape[0] / max(1,node_count))\n",
    "                total += np.sum(node_samples) * n_features\n",
    "        else:\n",
    "            total = np.nan\n",
    "    except Exception as e:\n",
    "        print('FLOPS estimation error:', e)\n",
    "        total = np.nan\n",
    "    return total\n",
    "\n",
    "# Estimate FLOPS for xgboost by summing 'cover' from booster trees\n",
    "import xgboost as xgb\n",
    "\n",
    "def estimate_flops_xgboost(xgb_model, n_features):\n",
    "    try:\n",
    "        booster = xgb_model.get_booster()\n",
    "        df_nodes = booster.trees_to_dataframe()\n",
    "        cover_col = None\n",
    "        for c in df_nodes.columns:\n",
    "            if 'cover' in c.lower():\n",
    "                cover_col = c\n",
    "                break\n",
    "        if cover_col is None:\n",
    "            return np.nan\n",
    "        covers = df_nodes[cover_col].astype(float).fillna(0.0)\n",
    "        total = covers.sum() * n_features\n",
    "        return total\n",
    "    except Exception as e:\n",
    "        print('XGBoost FLOPS estimation error:', e)\n",
    "        return np.nan\n",
    "\n",
    "print('Helpers ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d418cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original y_train shape: (6694,)\n",
      "Original y_test shape: (2870,)\n",
      "Sample y_train values:\n",
      "[1 0 1 0 1]\n",
      "Final y_train_1d shape: (6694,)\n",
      "Unique classes in y_train_1d: [0 1]\n",
      "Unique classes in y_test_1d: [0 1]\n",
      "\n",
      "====== RandomForest ======\n",
      "RandomForest done: acc=0.822, f1=0.822, train_time=1.44s, inf_per_sample=0.045ms\n",
      "\n",
      "====== ExtraTrees ======\n",
      "ExtraTrees done: acc=0.814, f1=0.814, train_time=1.06s, inf_per_sample=0.041ms\n",
      "\n",
      "====== AdaBoost ======\n",
      "AdaBoost done: acc=0.822, f1=0.822, train_time=2.45s, inf_per_sample=0.017ms\n",
      "\n",
      "====== RandomSubspace ======\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_parallel_build_estimators() got an unexpected keyword argument 'fit_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\nTypeError: _parallel_build_estimators() got an unexpected keyword argument 'fit_params'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Train - use y_train_1d\u001b[39;00m\n\u001b[0;32m     65\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m---> 66\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_1d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m train_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Test (predict)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:663\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    658\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    659\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    660\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    661\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    662\u001b[0m         )\n\u001b[1;32m--> 663\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:389\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;66;03m# Remap output\u001b[39;00m\n\u001b[0;32m    388\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_samples \u001b[38;5;241m=\u001b[39m n_samples\n\u001b[0;32m    390\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_y(y)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;66;03m# Check parameters\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:547\u001b[0m, in \u001b[0;36m_fit\u001b[1;34m(self, X, y, max_samples, max_depth, check_input, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaggingClassifier\u001b[39;00m(_RoutingNotSupportedMixin, ClassifierMixin, BaseBagging):\n\u001b[0;32m    542\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A Bagging classifier.\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    A Bagging classifier is an ensemble meta-estimator that fits base\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    classifiers each on random subsets of the original dataset and then\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    aggregate their individual predictions (either by voting or by averaging)\u001b[39;00m\n\u001b[1;32m--> 547\u001b[0m \u001b[38;5;124;03m    to form a final prediction. Such a meta-estimator can typically be used as\u001b[39;00m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;124;03m    a way to reduce the variance of a black-box estimator (e.g., a decision\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;124;03m    tree), by introducing randomization into its construction procedure and\u001b[39;00m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;124;03m    then making an ensemble out of it.\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    This algorithm encompasses several works from the literature. When random\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m    subsets of the dataset are drawn as random subsets of the samples, then\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m    this algorithm is known as Pasting [1]_. If samples are drawn with\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    replacement, then the method is known as Bagging [2]_. When random subsets\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;124;03m    of the dataset are drawn as random subsets of the features, then the method\u001b[39;00m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;124;03m    is known as Random Subspaces [3]_. Finally, when base estimators are built\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03m    on subsets of both samples and features, then the method is known as\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m    Random Patches [4]_.\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \n\u001b[0;32m    561\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <bagging>`.\u001b[39;00m\n\u001b[0;32m    562\u001b[0m \n\u001b[0;32m    563\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.15\u001b[39;00m\n\u001b[0;32m    564\u001b[0m \n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;124;03m    estimator : object, default=None\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03m        The base estimator to fit on random subsets of the dataset.\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m        If None, then the base estimator is a\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03m        :class:`~sklearn.tree.DecisionTreeClassifier`.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \n\u001b[0;32m    572\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.2\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03m           `base_estimator` was renamed to `estimator`.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \n\u001b[0;32m    575\u001b[0m \u001b[38;5;124;03m    n_estimators : int, default=10\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m        The number of base estimators in the ensemble.\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \n\u001b[0;32m    578\u001b[0m \u001b[38;5;124;03m    max_samples : int or float, default=1.0\u001b[39;00m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;124;03m        The number of samples to draw from X to train each base estimator (with\u001b[39;00m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03m        replacement by default, see `bootstrap` for more details).\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \n\u001b[0;32m    582\u001b[0m \u001b[38;5;124;03m        - If int, then draw `max_samples` samples.\u001b[39;00m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;124;03m        - If float, then draw `max_samples * X.shape[0]` samples.\u001b[39;00m\n\u001b[0;32m    584\u001b[0m \n\u001b[0;32m    585\u001b[0m \u001b[38;5;124;03m    max_features : int or float, default=1.0\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;124;03m        The number of features to draw from X to train each base estimator (\u001b[39;00m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;124;03m        without replacement by default, see `bootstrap_features` for more\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;124;03m        details).\u001b[39;00m\n\u001b[0;32m    589\u001b[0m \n\u001b[0;32m    590\u001b[0m \u001b[38;5;124;03m        - If int, then draw `max_features` features.\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m        - If float, then draw `max(1, int(max_features * n_features_in_))` features.\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \n\u001b[0;32m    593\u001b[0m \u001b[38;5;124;03m    bootstrap : bool, default=True\u001b[39;00m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;124;03m        Whether samples are drawn with replacement. If False, sampling\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03m        without replacement is performed.\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m    bootstrap_features : bool, default=False\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m        Whether features are drawn with replacement.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \n\u001b[0;32m    600\u001b[0m \u001b[38;5;124;03m    oob_score : bool, default=False\u001b[39;00m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;124;03m        Whether to use out-of-bag samples to estimate\u001b[39;00m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;124;03m        the generalization error. Only available if bootstrap=True.\u001b[39;00m\n\u001b[0;32m    603\u001b[0m \n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m    warm_start : bool, default=False\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;124;03m        When set to True, reuse the solution of the previous call to fit\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[38;5;124;03m        and add more estimators to the ensemble, otherwise, just fit\u001b[39;00m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;124;03m        a whole new ensemble. See :term:`the Glossary <warm_start>`.\u001b[39;00m\n\u001b[0;32m    608\u001b[0m \n\u001b[0;32m    609\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.17\u001b[39;00m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;124;03m           *warm_start* constructor parameter.\u001b[39;00m\n\u001b[0;32m    611\u001b[0m \n\u001b[0;32m    612\u001b[0m \u001b[38;5;124;03m    n_jobs : int, default=None\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \u001b[38;5;124;03m        The number of jobs to run in parallel for both :meth:`fit` and\u001b[39;00m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;124;03m        :meth:`predict`. ``None`` means 1 unless in a\u001b[39;00m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;124;03m        :obj:`joblib.parallel_backend` context. ``-1`` means using all\u001b[39;00m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;124;03m        processors. See :term:`Glossary <n_jobs>` for more details.\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \n\u001b[0;32m    618\u001b[0m \u001b[38;5;124;03m    random_state : int, RandomState instance or None, default=None\u001b[39;00m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;124;03m        Controls the random resampling of the original dataset\u001b[39;00m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;124;03m        (sample wise and feature wise).\u001b[39;00m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m        If the base estimator accepts a `random_state` attribute, a different\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m        seed is generated for each instance in the ensemble.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m        Pass an int for reproducible output across multiple function calls.\u001b[39;00m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;124;03m        See :term:`Glossary <random_state>`.\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \n\u001b[0;32m    626\u001b[0m \u001b[38;5;124;03m    verbose : int, default=0\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m        Controls the verbosity when fitting and predicting.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \n\u001b[0;32m    629\u001b[0m \u001b[38;5;124;03m    Attributes\u001b[39;00m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;124;03m    estimator_ : estimator\u001b[39;00m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;124;03m        The base estimator from which the ensemble is grown.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m \n\u001b[0;32m    634\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.2\u001b[39;00m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;124;03m           `base_estimator_` was renamed to `estimator_`.\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \n\u001b[0;32m    637\u001b[0m \u001b[38;5;124;03m    n_features_in_ : int\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m        Number of features seen during :term:`fit`.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 0.24\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m    feature_names_in_ : ndarray of shape (`n_features_in_`,)\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;124;03m        Names of features seen during :term:`fit`. Defined only when `X`\u001b[39;00m\n\u001b[0;32m    644\u001b[0m \u001b[38;5;124;03m        has feature names that are all strings.\u001b[39;00m\n\u001b[0;32m    645\u001b[0m \n\u001b[0;32m    646\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.0\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \n\u001b[0;32m    648\u001b[0m \u001b[38;5;124;03m    estimators_ : list of estimators\u001b[39;00m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;124;03m        The collection of fitted base estimators.\u001b[39;00m\n\u001b[0;32m    650\u001b[0m \n\u001b[0;32m    651\u001b[0m \u001b[38;5;124;03m    estimators_samples_ : list of arrays\u001b[39;00m\n\u001b[0;32m    652\u001b[0m \u001b[38;5;124;03m        The subset of drawn samples (i.e., the in-bag samples) for each base\u001b[39;00m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;124;03m        estimator. Each subset is defined by an array of the indices selected.\u001b[39;00m\n\u001b[0;32m    654\u001b[0m \n\u001b[0;32m    655\u001b[0m \u001b[38;5;124;03m    estimators_features_ : list of arrays\u001b[39;00m\n\u001b[0;32m    656\u001b[0m \u001b[38;5;124;03m        The subset of drawn features for each base estimator.\u001b[39;00m\n\u001b[0;32m    657\u001b[0m \n\u001b[0;32m    658\u001b[0m \u001b[38;5;124;03m    classes_ : ndarray of shape (n_classes,)\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[38;5;124;03m        The classes labels.\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    n_classes_ : int or list\u001b[39;00m\n\u001b[0;32m    662\u001b[0m \u001b[38;5;124;03m        The number of classes.\u001b[39;00m\n\u001b[0;32m    663\u001b[0m \n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m    oob_score_ : float\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m        Score of the training dataset obtained using an out-of-bag estimate.\u001b[39;00m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;124;03m        This attribute exists only when ``oob_score`` is True.\u001b[39;00m\n\u001b[0;32m    667\u001b[0m \n\u001b[0;32m    668\u001b[0m \u001b[38;5;124;03m    oob_decision_function_ : ndarray of shape (n_samples, n_classes)\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;124;03m        Decision function computed with out-of-bag estimate on the training\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;124;03m        set. If n_estimators is small it might be possible that a data point\u001b[39;00m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;124;03m        was never left out during the bootstrap. In this case,\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;124;03m        `oob_decision_function_` might contain NaN. This attribute exists\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;124;03m        only when ``oob_score`` is True.\u001b[39;00m\n\u001b[0;32m    674\u001b[0m \n\u001b[0;32m    675\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    677\u001b[0m \u001b[38;5;124;03m    BaggingRegressor : A Bagging regressor.\u001b[39;00m\n\u001b[0;32m    678\u001b[0m \n\u001b[0;32m    679\u001b[0m \u001b[38;5;124;03m    References\u001b[39;00m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \n\u001b[0;32m    682\u001b[0m \u001b[38;5;124;03m    .. [1] L. Breiman, \"Pasting small votes for classification in large\u001b[39;00m\n\u001b[0;32m    683\u001b[0m \u001b[38;5;124;03m           databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\u001b[39;00m\n\u001b[0;32m    684\u001b[0m \n\u001b[0;32m    685\u001b[0m \u001b[38;5;124;03m    .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[38;5;124;03m           1996.\u001b[39;00m\n\u001b[0;32m    687\u001b[0m \n\u001b[0;32m    688\u001b[0m \u001b[38;5;124;03m    .. [3] T. Ho, \"The random subspace method for constructing decision\u001b[39;00m\n\u001b[0;32m    689\u001b[0m \u001b[38;5;124;03m           forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;124;03m           1998.\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \n\u001b[0;32m    692\u001b[0m \u001b[38;5;124;03m    .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m           Learning and Knowledge Discovery in Databases, 346-361, 2012.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.svm import SVC\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.ensemble import BaggingClassifier\u001b[39;00m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.datasets import make_classification\u001b[39;00m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;124;03m    >>> X, y = make_classification(n_samples=100, n_features=4,\u001b[39;00m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;124;03m    ...                            n_informative=2, n_redundant=0,\u001b[39;00m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;124;03m    ...                            random_state=0, shuffle=False)\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;124;03m    >>> clf = BaggingClassifier(estimator=SVC(),\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;124;03m    ...                         n_estimators=10, random_state=0).fit(X, y)\u001b[39;00m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    >>> clf.predict([[0, 0, 0, 0]])\u001b[39;00m\n\u001b[0;32m    706\u001b[0m \u001b[38;5;124;03m    array([1])\u001b[39;00m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    711\u001b[0m         estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    722\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    723\u001b[0m     ):\n\u001b[0;32m    724\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    725\u001b[0m             estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    726\u001b[0m             n_estimators\u001b[38;5;241m=\u001b[39mn_estimators,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    735\u001b[0m             verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    736\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[0m, in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelayed\u001b[39m(function):\n\u001b[0;32m     72\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Decorator used to capture the arguments of a function.\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \n\u001b[0;32m     74\u001b[0m \u001b[38;5;124;03m    This alternative to `joblib.delayed` is meant to be used in conjunction\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m    with `sklearn.utils.parallel.Parallel`. The latter captures the scikit-\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m    learn configuration by calling `sklearn.get_config()` in the current\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    thread, prior to dispatching the first task. The captured configuration is\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m    then propagated and enabled for the duration of the execution of the\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    delayed function in the joblib workers.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    .. versionchanged:: 1.3\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;124;03m       `delayed` was moved from `sklearn.utils.fixes` to `sklearn.utils.parallel`\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m       in scikit-learn 1.3.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    function : callable\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m        The function to be delayed.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;124;03m    output: tuple\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;124;03m        Tuple containing the delayed function, the positional arguments, and the\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;124;03m        keyword arguments.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(function)\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelayed_function\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _FuncWrapper(function), args, kwargs\n",
      "File \u001b[1;32mc:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:445\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\RAJ DAVE\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: _parallel_build_estimators() got an unexpected keyword argument 'fit_params'"
     ]
    }
   ],
   "source": [
    "# Models to evaluate (matching Electronics paper families) and training loop\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import joblib, os, matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(n_estimators=500, class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    'ExtraTrees': ExtraTreesClassifier(n_estimators=500, class_weight='balanced', random_state=42, n_jobs=-1),\n",
    "    'AdaBoost': AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=3), n_estimators=200, random_state=42),\n",
    "    'RandomSubspace': BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=200, max_features=0.5, bootstrap=False, random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "# Stacking using RF and ET as base\n",
    "base_ests = [('rf', RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42, n_jobs=-1)),\n",
    "            ('et', ExtraTreesClassifier(n_estimators=200, class_weight='balanced', random_state=42, n_jobs=-1))]\n",
    "models['Stacking'] = StackingClassifier(estimators=base_ests, final_estimator=LogisticRegression(max_iter=1000), n_jobs=-1)\n",
    "\n",
    "# IMPORTANT: Convert y_train and y_test from multilabel-indicator to 1D arrays BEFORE training\n",
    "# Convert to numpy arrays if they're DataFrames\n",
    "y_train_arr = y_train.values if hasattr(y_train, 'values') else y_train\n",
    "y_test_arr = y_test.values if hasattr(y_test, 'values') else y_test\n",
    "\n",
    "# Diagnose the data format\n",
    "print(f\"Original y_train shape: {y_train_arr.shape}\")\n",
    "print(f\"Original y_test shape: {y_test_arr.shape}\")\n",
    "print(f\"Sample y_train values:\\n{y_train_arr[:5]}\")\n",
    "\n",
    "# Convert from 2D multilabel-indicator to 1D class labels\n",
    "if len(y_train_arr.shape) > 1 and y_train_arr.shape[1] > 1:\n",
    "    # Check if it's one-hot encoded (sum of each row should be 1)\n",
    "    row_sums = y_train_arr.sum(axis=1)\n",
    "    if np.allclose(row_sums, 1):\n",
    "        # One-hot encoded - use argmax\n",
    "        y_train_1d = y_train_arr.argmax(axis=1)\n",
    "        y_test_1d = y_test_arr.argmax(axis=1)\n",
    "        print(f\"Detected one-hot encoding, converted using argmax\")\n",
    "    else:\n",
    "        # True multilabel or different format - take first column or use different strategy\n",
    "        print(f\"Warning: Data appears to be true multilabel (row sums: {row_sums[:5]})\")\n",
    "        # Try taking the first column as the target\n",
    "        y_train_1d = y_train_arr[:, 0].ravel()\n",
    "        y_test_1d = y_test_arr[:, 0].ravel()\n",
    "        print(f\"Using first column as target\")\n",
    "else:\n",
    "    y_train_1d = y_train_arr.ravel()\n",
    "    y_test_1d = y_test_arr.ravel()\n",
    "\n",
    "print(f\"Final y_train_1d shape: {y_train_1d.shape}\")\n",
    "print(f\"Unique classes in y_train_1d: {np.unique(y_train_1d)}\")\n",
    "print(f\"Unique classes in y_test_1d: {np.unique(y_test_1d)}\")\n",
    "\n",
    "# Training loop\n",
    "results = []\n",
    "\n",
    "for name, clf in models.items():\n",
    "    print('\\n======', name, '======')\n",
    "    outdir = os.path.join('plots', name)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    pipe = make_pipeline(SimpleImputer(strategy='median'), StandardScaler(), clf)\n",
    "    \n",
    "    # Train - use y_train_1d\n",
    "    t0 = time.perf_counter()\n",
    "    pipe.fit(X_train, y_train_1d)\n",
    "    train_time = time.perf_counter() - t0\n",
    "    \n",
    "    # Test (predict)\n",
    "    t1 = time.perf_counter()\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    test_time = time.perf_counter() - t1\n",
    "    \n",
    "    # Inference timing\n",
    "    inf_total, inf_per_sample = measure_inference_time(pipe, X_test, repeats=3)\n",
    "    \n",
    "    # Metrics - use y_test_1d\n",
    "    acc = accuracy_score(y_test_1d, y_pred)\n",
    "    prec = precision_score(y_test_1d, y_pred, average='weighted', zero_division=0)\n",
    "    rec = recall_score(y_test_1d, y_pred, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y_test_1d, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # ROC AUC if possible\n",
    "    y_proba = None\n",
    "    try:\n",
    "        y_proba = pipe.predict_proba(X_test)\n",
    "        # For multiclass, use one-vs-rest\n",
    "        roc = roc_auc_score(y_test_1d, y_proba, multi_class='ovr', average='weighted')\n",
    "    except Exception:\n",
    "        roc = float('nan')\n",
    "    \n",
    "    # FLOPS estimate\n",
    "    n_features = X_train.shape[1]\n",
    "    flops = np.nan\n",
    "    try:\n",
    "        final_est = pipe.named_steps[list(pipe.named_steps.keys())[-1]]\n",
    "        if name in ['RandomForest','ExtraTrees','RandomSubspace','AdaBoost']:\n",
    "            flops = estimate_flops_sklearn_ensemble(final_est, n_features)\n",
    "        elif name == 'Stacking':\n",
    "            total = 0.0\n",
    "            if hasattr(final_est, 'estimators_'):\n",
    "                for e in final_est.estimators_:\n",
    "                    total += estimate_flops_sklearn_ensemble(e, n_features)\n",
    "            flops = total\n",
    "    except Exception as e:\n",
    "        print('FLOPS estimation failed for', name, e)\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(pipe, os.path.join('models', f'model_{name}.joblib'))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test_1d, y_pred)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion: {name}')\n",
    "    plt.savefig(os.path.join(outdir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # ROC curve\n",
    "    if not np.isnan(roc):\n",
    "        from sklearn.metrics import roc_curve\n",
    "        # For multiclass ROC, plot one-vs-rest for each class\n",
    "        n_classes = y_proba.shape[1]\n",
    "        plt.figure()\n",
    "        for i in range(min(n_classes, 5)):  # Plot max 5 classes to avoid clutter\n",
    "            fpr, tpr, _ = roc_curve((y_test_1d == i).astype(int), y_proba[:, i])\n",
    "            plt.plot(fpr, tpr, label=f'Class {i}')\n",
    "        plt.plot([0,1],[0,1],'k--')\n",
    "        plt.legend()\n",
    "        plt.title(f'ROC: {name} (AUC={roc:.3f})')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.savefig(os.path.join(outdir, 'roc.png'))\n",
    "        plt.close()\n",
    "    \n",
    "    # Feature importances\n",
    "    try:\n",
    "        final_est = pipe.named_steps[list(pipe.named_steps.keys())[-1]]\n",
    "        if hasattr(final_est, 'feature_importances_'):\n",
    "            fi = final_est.feature_importances_\n",
    "            importances = pd.Series(fi, index=FEATURE_COLS).sort_values(ascending=False)\n",
    "            plt.figure(figsize=(6,4))\n",
    "            importances[:30].plot.barh()\n",
    "            plt.title(f'Feature importances: {name}')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(outdir, 'feature_importances.png'))\n",
    "            plt.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    results.append({\n",
    "        'model': name,\n",
    "        'n_features': n_features,\n",
    "        'train_time_s': train_time,\n",
    "        'test_time_s': test_time,\n",
    "        'inference_time_total_s': inf_total,\n",
    "        'inference_time_per_sample_s': inf_per_sample,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc,\n",
    "        'flops_estimate': flops,\n",
    "        'n_train': X_train.shape[0],\n",
    "        'n_test': X_test.shape[0]\n",
    "    })\n",
    "    print(f\"{name} done: acc={acc:.3f}, f1={f1:.3f}, train_time={train_time:.2f}s, inf_per_sample={inf_per_sample*1e3:.3f}ms\")\n",
    "\n",
    "# Save results\n",
    "import pandas as pd\n",
    "resdf = pd.DataFrame(results)\n",
    "resdf.to_csv('results/electronics_ensemble_results.csv', index=False)\n",
    "print('Saved results to results/electronics_ensemble_results.csv')\n",
    "resdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e940b",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- The `flops_estimate` is an approximate comparative metric, not exact FLOPS. You can profile more accurately with OS/hardware profilers.\n",
    "- To match the Electronics paper exactly, consider applying their filtering steps (they balanced candidate vs confirmed to 3178 rows); you can undersample/oversample using imblearn or the helper functions from earlier notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d977a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded KOI: (9564, 8)\n",
      "Columns (first 50): ['kepoi_name', 'koi_disposition', 'koi_pdisposition', 'koi_period', 'koi_duration', 'koi_prad', 'koi_depth', 'koi_model_snr']\n",
      "Using label column: koi_disposition\n",
      "Task: Planet (CANDIDATE/CONFIRMED) vs FALSE POSITIVE\n",
      "After label mapping, shape = (9564, 9)\n",
      "label\n",
      "0    4839\n",
      "1    4725\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Selected 5 features (filtered to avoid leakage)\n",
      "Features: ['koi_period', 'koi_duration', 'koi_prad', 'koi_depth', 'koi_model_snr']\n",
      "\n",
      "Final dataset shape: (9564, 6)\n",
      "\n",
      "y shape: (9564,) (should be 1D)\n",
      "y unique values: [1 0]\n",
      "y value counts:\n",
      "label\n",
      "0    4839\n",
      "1    4725\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Train: (6694, 5), Test: (2870, 5)\n",
      "Train class balance: [3387 3307]\n",
      "Test class balance: [1452 1418]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
